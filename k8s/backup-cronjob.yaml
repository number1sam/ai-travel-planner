apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: travel-agent
spec:
  schedule: "0 2 * * *" # Daily at 2 AM
  concurrencyPolicy: Replace
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: postgres-backup
            image: postgres:15-alpine
            command:
            - /bin/sh
            - -c
            - |
              set -e
              BACKUP_FILE="/backups/postgres-backup-$(date +%Y%m%d-%H%M%S).sql"
              echo "Starting backup to $BACKUP_FILE"
              
              # Create backup
              pg_dump $DATABASE_URL > $BACKUP_FILE
              
              # Compress backup
              gzip $BACKUP_FILE
              
              # Upload to S3 (optional)
              if [ -n "$AWS_ACCESS_KEY_ID" ]; then
                aws s3 cp ${BACKUP_FILE}.gz s3://$AWS_BACKUP_BUCKET/postgres/
              fi
              
              # Clean up old backups (keep last 30 days)
              find /backups -name "postgres-backup-*.sql.gz" -mtime +30 -delete
              
              echo "Backup completed successfully"
            env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: DATABASE_URL
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: AWS_ACCESS_KEY_ID
                  optional: true
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: AWS_SECRET_ACCESS_KEY
                  optional: true
            - name: AWS_BACKUP_BUCKET
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: AWS_BACKUP_BUCKET
                  optional: true
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "500m"
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: travel-agent
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: standard
---
# Redis backup job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: redis-backup
  namespace: travel-agent
spec:
  schedule: "0 3 * * *" # Daily at 3 AM
  concurrencyPolicy: Replace
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: redis-backup
            image: redis:7-alpine
            command:
            - /bin/sh
            - -c
            - |
              set -e
              BACKUP_FILE="/backups/redis-backup-$(date +%Y%m%d-%H%M%S).rdb"
              echo "Starting Redis backup to $BACKUP_FILE"
              
              # Create Redis backup
              redis-cli -h redis -p 6379 --rdb $BACKUP_FILE
              
              # Compress backup
              gzip $BACKUP_FILE
              
              # Upload to S3 (optional)
              if [ -n "$AWS_ACCESS_KEY_ID" ]; then
                aws s3 cp ${BACKUP_FILE}.gz s3://$AWS_BACKUP_BUCKET/redis/
              fi
              
              # Clean up old backups (keep last 30 days)
              find /backups -name "redis-backup-*.rdb.gz" -mtime +30 -delete
              
              echo "Redis backup completed successfully"
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: AWS_ACCESS_KEY_ID
                  optional: true
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: AWS_SECRET_ACCESS_KEY
                  optional: true
            - name: AWS_BACKUP_BUCKET
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: AWS_BACKUP_BUCKET
                  optional: true
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
            resources:
              requests:
                memory: "128Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "250m"
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
---
# Security cleanup job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: security-cleanup
  namespace: travel-agent
spec:
  schedule: "0 4 * * *" # Daily at 4 AM
  concurrencyPolicy: Replace
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: security-cleanup
            image: travel-agent:latest
            command:
            - /bin/sh
            - -c
            - |
              echo "Starting security cleanup tasks"
              
              # Run GDPR cleanup
              node -e "
                const { GDPRService } = require('./dist/services/GDPRService');
                const { SecurityCronService } = require('./dist/services/SecurityCronService');
                
                async function cleanup() {
                  try {
                    console.log('Running GDPR cleanup...');
                    await GDPRService.cleanupExpiredData();
                    
                    console.log('Processing scheduled deletions...');
                    await GDPRService.processScheduledDeletions();
                    
                    console.log('Security cleanup completed successfully');
                    process.exit(0);
                  } catch (error) {
                    console.error('Security cleanup failed:', error);
                    process.exit(1);
                  }
                }
                
                cleanup();
              "
            env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: travel-agent-secrets
                  key: DATABASE_URL
            envFrom:
            - configMapRef:
                name: travel-agent-config
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "250m"